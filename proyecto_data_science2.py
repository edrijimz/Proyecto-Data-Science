# -*- coding: utf-8 -*-
"""Proyecto Data Science2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbOi7p0xIb9I0XaTAw55cCDU4bN2SmDc
"""

from google.colab import drive
drive.mount("/content/drive")


#medals_url = "https://github.com/samuelsaldanav/datasets/blob/d4d2c665362c034bbfff5a977faf8b23f8b5924a/House%20Prices%20-%20Advanced%20Regression%20Techniques/House-Prices-Advanced.csv"

import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

mainpath = ("/content/drive/MyDrive/Colab Notebooks/Proyecto Data Science/")
filename = "adult.csv"
fullpath = os.path.join(mainpath, filename)

data = pd.read_csv(fullpath)
data.head()
print(data.shape)

def buscar_dummy_columns(data, dummy_value='?'):
    dummy_columns = []
    for col in data.columns:
        if data[data[col] == dummy_value].shape[0] > 0:
            dummy_columns.append(col)
    return dummy_columns

dummy_columns = buscar_dummy_columns(data)

print("Columnas con dummies:")
for col in dummy_columns:
    print(f"Columna '{col}' tiene dummies.")

data.head()

valor_dummy = '?'
valores_dummy = (data == valor_dummy).sum()

# Convertir la Serie a un DataFrame
valores_dummy = pd.DataFrame(valores_dummy, columns=['count'])

# Calcular el porcentaje de valores "?" en cada columna
sum_total = len(data)
valores_dummy['percentage'] = (valores_dummy['count'] / sum_total) * 100

# Redondear los porcentajes a tres decimales
valores_dummy['percentage'] = round(valores_dummy['percentage'], 3)

# Ordenar por el porcentaje de valores "?" en orden descendente
valores_dummy = valores_dummy.sort_values('percentage', ascending=False)

print(valores_dummy)

plt.figure(figsize=(10, 3))
sns.barplot(x=valores_dummy.index, y=valores_dummy['count'], palette='viridis')

plt.title('Cantidad de Valores "Dummy" (?) por Columna')
plt.xlabel('Columnas')
plt.ylabel('Cantidad de Valores "Dummy"')
plt.xticks(rotation=45)
plt.show()

# Plotear el porcentaje
plt.figure(figsize=(10, 3))
sns.barplot(x=valores_dummy.index, y=valores_dummy['percentage'], palette='viridis')

plt.title('Porcentaje de Valores "Dummy" (?) por Columna')
plt.xlabel('Columnas')
plt.ylabel('Porcentaje de Valores "Dummy" (%)')
plt.xticks(rotation=45)
plt.show()

bool_df = data.applymap(lambda x: x == '?')
buscar_vacios = bool_df.sum()
print("\nNúmero de veces que '?' aparece en cada columna:")

dummy_data = pd.DataFrame(list(dummy_counts.items()), columns=['Column', 'Dummy Count'])
dummy_data['Percentage'] = dummy_data['Dummy Count'] / len(data) * 100

plt.figure(figsize=(10, 6))
sns.barplot(x='Column', y='Percentage', data=dummy_data, palette='viridis')
plt.title('Porcentaje de Datos Dummies por Columna')
plt.ylabel('Porcentaje')
plt.xlabel('Columna')
plt.show()

data_dummies = pd.get_dummies(data, columns=['marital.status'])
print(data_dummies)

def show_dummy_values(data, column, dummy_value='?'):
    # Filtrar las filas que contienen el valor dummy
    dummy_rows = data[data[column] == dummy_value]
    if not dummy_rows.empty:
        print(f"\nFilas que contienen el valor dummy '{dummy_value}' en la columna '{column}':")
        print(dummy_rows)
    else:
        print(f"\nNo se encontraron valores dummy '{dummy_value}' en la columna '{column}'.")

# Buscar valores dummy en cada columna del DataFrame
for col in data.columns:
    show_dummy_values(data, col)

print(data_dummies)

data.head()

def show_dummy_values(data, column, dummy_value='?'):
    # Filtrar las filas que contienen el valor dummy
    dummy_rows = data[data[column] == dummy_value]
    if not dummy_rows.empty:
        print(f"\nFilas que contienen el valor dummy '{dummy_value}' en la columna '{column}':")
        print(dummy_rows)
    else:
        print(f"\nNo se encontraron valores dummy '{dummy_value}' en la columna '{column}'.")

# Buscar valores dummy en cada columna del DataFrame
for col in data.columns:
    show_dummy_values(data, col)

show_dummy_values(data, 'occupation')

def compare_dummy_values(data, column, dummy_value='?'):
    # Filtrar las filas que contienen el valor dummy
    dummy_rows = data[data[column] == dummy_value]
    dummy_count = dummy_rows.shape[0]

    # Filtrar las filas que no contienen el valor dummy
    non_dummy_rows = data[data[column] != dummy_value]
    non_dummy_count = non_dummy_rows.shape[0]

    # Mostrar resultados
    print(f"\nComparación para la columna '{column}':")
    print(f"- Valores dummy '{dummy_value}': {dummy_count} filas")
    print(f"- Valores no dummy: {non_dummy_count} filas")

    # Mostrar algunas filas con valores dummy
    if dummy_count > 0:
        print(f"\nEjemplos de filas con valor dummy '{dummy_value}':")
        print(dummy_rows.head(5))  # Mostrar solo las primeras 5 filas

    # Mostrar algunas filas sin valores dummy
    if non_dummy_count > 0:
        print(f"\nEjemplos de filas sin valor dummy en la columna '{column}':")
        print(non_dummy_rows.head(5))  # Mostrar solo las primeras 5 filas

compare_dummy_values(data, 'workclass')

def show_dummy_values(data, column, dummy_value='?'):

    dummy_rows = data[data[column] == dummy_value]
    dummy_count = dummy_rows.shape[0]
    if dummy_count > 0:
        print(f"\nColumna '{column}' tiene {dummy_count} valor(es) dummy '{dummy_value}'.")
        print(dummy_rows)
    else:
        print(f"\nNo se encontraron valores dummy '{dummy_value}' en la columna '{column}'.")

for col in data.columns:
    show_dummy_values(data, col)

def show_dummy_values(data, column, dummy_value='?'):
    # Filtrar las filas que contienen el valor dummy
    dummy_rows = data[data[column] == dummy_value]
    if not dummy_rows.empty:
        print(f"\nFilas que contienen el valor dummy '{dummy_value}' en la columna '{column}':")
        print(dummy_rows)
    else:
        print(f"\nNo se encontraron valores dummy '{dummy_value}' en la columna '{column}'.")

# Buscar valores dummy en cada columna del DataFrame
for col in data.columns:
    show_dummy_values(data, col)

data.info()

data.columns

data["age"].describe()
data["age"].unique()
data["age"].mode()
print("El más común es", data["age"].mode())
#print(data["age"].unique())


val_unique = data["age"].unique()
val_unique.sort()  # Ordenarlos
val_unique = val_unique[::-1]
print(val_unique)
#

data["workclass"].describe()
data["workclass"].unique()
data["workclass"].mode()
print("El más común es", data["workclass"].mode())
print(data["workclass"].unique())
unm = data["workclass"].value_counts().max
print(unm)

data["education"].describe()
data["education"].unique()
data["education"].mode()
print("El más común es", data["education"].mode())
unm = data["education"].value_counts().max
print(unm)
print(data["education"].unique())

data['marital.status'].describe()
data['marital.status'].unique()
data['marital.status'].mode()
print("El más común es", data['marital.status'].mode())
print(data['marital.status'].unique())
unm = data["marital.status"].value_counts().max
print(unm)

data['relationship'].describe()
data['relationship'].unique()
data["relationship"].value_counts()["Wife"]
data['relationship'].mode()
print("El más común es", data['relationship'].mode())
print(data['relationship'].unique())
unm = data["relationship"].value_counts().idxmin
print(unm)

data["race"].describe()
data["race"].unique()
data['race'].mode()
print("El más común es", data['race'].mode())
print(data['race'].unique())
unm = data["race"].value_counts().max
print(unm)

data["sex"].describe()
data["sex"].unique()
data['sex'].mode()
print("El más común es", data['sex'].mode())
print(data['sex'].unique())
unm = data["sex"].value_counts().max
print(unm)

data["hours.per.week"].describe()
data["hours.per.week"].unique()
data["hours.per.week"].mode()
print("El más común es", data["hours.per.week"].mode())
#print(data["age"].unique())


val_unique = data["hours.per.week"].unique()
val_unique.sort()  # Ordenarlos
val_unique = val_unique[::-1]
print(val_unique)
unm = data["hours.per.week"].value_counts().min
print("El orden es ", unm)

data["native.country"].describe()
data["native.country"].unique()
data['native.country'].mode()
print("El más común es", data['native.country'].mode())
print(data['native.country'].unique())
unm = data["native.country"].value_counts().min
print(unm)

data["income"].describe()
data["income"].unique()
data["income"].mode()
print("El más común es", data["income"].mode())
unm = data["income"].value_counts().max
print(unm)

data.select_dtypes(include=['int64', 'float64'])

data.groupby(['education', 'workclass']).size().unstack(fill_value=0)

data.groupby(['education', 'income']).size().unstack(fill_value=0)

data.groupby(['occupation', 'workclass']).size().unstack(fill_value=0)

data.groupby(['occupation', 'income']).size().unstack(fill_value=0)

data.groupby(['marital.status', 'income']).size().unstack(fill_value=0)

data.groupby(['age', 'income']).size().unstack(fill_value=0)

data.groupby(['race', 'income']).size().unstack(fill_value=0)

data.groupby(['education', 'race']).size().unstack(fill_value=0)

data.groupby(['occupation', 'race']).size().unstack(fill_value=0)

# Crear un DataFrame booleano donde True indica la presencia de "?"
bool_df = data.applymap(lambda x: x == '?')

# Sumar los valores True por columna para contar las ocurrencias de "?"
buscar_vacios = bool_df.sum()

print("\nNúmero de veces que '?' aparece en cada columna:")
print(buscar_vacios)

plt.figure(figsize=(18, 6))
sns.countplot(data=data, x='education', hue='occupation', palette='viridis')
plt.title('Comparación entre Educación y Ocupación')
plt.xlabel('Educación')
plt.ylabel('Conteo')
plt.legend(title='Ocupación')
plt.show()

plt.figure(figsize=(16, 6))
sns.countplot(data=data, x='education', hue='income', palette='viridis')
plt.title('Comparación entre Educación e Ingreso')
plt.xlabel('Educación')
plt.ylabel('Conteo')
plt.legend(title='Ingreso')
plt.show()

plt.figure(figsize=(18, 6))
sns.countplot(data=data, x='occupation', hue='workclass', palette='viridis')
plt.title('Comparación entre Ocupación y Workclass')
plt.xlabel('Ocupación')
plt.ylabel('Conteo')
plt.legend(title='Workclass')
plt.show()

plt.figure(figsize=(22, 6))
sns.countplot(data=data, x='occupation', hue='income', palette='viridis')
plt.title('Comparación entre Ocupación e Ingreso')
plt.xlabel('Ocupación')
plt.ylabel('Conteo')
plt.legend(title='Ingreso')
plt.show()

plt.figure(figsize=(14, 6))
sns.countplot(data=data, x='marital.status', hue='income', palette='viridis')
plt.title('Comparación entre Marital Status e Income')
plt.xlabel('Marital Status')
plt.ylabel('Conteo')
plt.legend(title='Ingreso')
plt.show()

plt.figure(figsize=(24, 6))
sns.countplot(data=data, x='age', hue='income', palette='viridis')
plt.title('Comparación entre Edad e Ingreso')
plt.xlabel('Edad')
plt.ylabel('Conteo')
plt.legend(title='Ingreso')
plt.show()

plt.figure(figsize=(18, 6))
sns.countplot(data=data, x='education', hue='race', palette='viridis')
plt.title('Comparación entre Educación y Raza')
plt.xlabel('Educación')
plt.ylabel('Conteo')
plt.legend(title='Raza')
plt.show()

plt.figure(figsize=(22, 6))
sns.countplot(data=data, x='occupation', hue='race', palette='viridis')
plt.title('Comparación entre Ocupación y Raza')
plt.xlabel('Ocupación')
plt.ylabel('Conteo')
plt.legend(title='Raza')
plt.show()

data.isnull().sum()

data.isnull().sum()

data.duplicated()

data.duplicated().sum()

data[data.duplicated()]

data.info()

data['native.country']=data['native.country'].replace('?', 'Unknown')

data['workclass'] = data['workclass'].replace('?', 'Unknown')

data['occupation']=data['occupation'].replace('?', 'Unknown')

data_dummies = pd.get_dummies(data, columns=['education', 'workclass', 'income'])
print(data_dummies)

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Calcular la matriz de correlación
corr_matrix = numeric_cols.corr()

# Graficar el heatmap de la matriz de correlación
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Heatmap de la Matriz de Correlación')
plt.show()

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Calcular la matriz de correlación
df1_corr_mat = numeric_cols.corr()

# Graficar el heatmap de la matriz de correlación
fig, ax = plt.subplots(figsize=(10, 8))  # Ajustar el tamaño de la figura para una mejor visualización
sns.heatmap(df1_corr_mat, annot=True, cmap='YlOrBr', ax=ax)
plt.title('Heatmap de la Matriz de Correlación')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='race', hue='education', palette='viridis')
plt.title('Comparación entre Raza y Educación')
plt.xlabel('Raza')
plt.ylabel('Conteo')
plt.legend(title='Educación')
plt.show()

plt.figure(figsize=(18, 6))
sns.countplot(data=data, x='education', hue='occupation', palette='viridis')
plt.title('Comparación entre Education y Ocupación')
plt.xlabel('Educación')
plt.ylabel('Conteo')
plt.legend(title='Ocupación')
plt.show()

plt.figure(figsize=(18, 6))
sns.countplot(data=data, x='occupation', hue='workclass', palette='viridis')
plt.title('Comparación entre Educación e Income')
plt.xlabel('Educación')
plt.ylabel('Conteo')
plt.legend(title='Income')
plt.show()

data["race"].describe()

data_dummies = pd.get_dummies(data, columns=['education', 'workclass', 'income'])

data.drop(columns=['fnlwgt'], inplace=True)

data.head()

data.shape

data["age"].describe()

data["income"].describe()

data["age"].describe()

data["education"].describe()

data["workclass"].describe()

data["marital.status"].describe()

data["relationship"].describe()

data["hours.per.week"].describe()

data.head()

sns.pairplot(data)

data.head()

data.columns

"""PCA"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import scale
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

data_cov = pd.get_dummies(data)

# Calculate the covariance matrix on the encoded data
data_cov = data_cov.cov()  # Use the .cov() method on the DataFrame
print(data_cov)

plt.figure(figsize=(12, 10))
sns.heatmap(data_cov, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Covarianza')
plt.show()

ss = StandardScaler()
x_cols = ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']
X_subset = data[x_cols]
data[x_cols] = ss.fit_transform(X_subset)
print(data)

ss = StandardScaler()
x_cols = ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']
X_subset = data[x_cols]
data[x_cols] = ss.fit_transform(X_subset)
print(data)

numeric_cols = data.select_dtypes(include=['int64', 'float64'])


# Aplicar PCA
pca = PCA(n_components=2)  # Elegir 2 componentes principales para visualizar en 2D
pca_result = pca.fit_transform(scaled_data)

# Crear un DataFrame con los resultados del PCA
pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Verificar si 'income' existe en el DataFrame original y renombrarlo si es necesario
if 'income' in data.columns:
    pca_data['income'] = data['income']
elif 'target' in data.columns: # Check if the column is named 'target' instead
    pca_data['income'] = data['target']
else:
    print("Warning: No 'income' or 'target' column found. The scatterplot will not be colored by class.")

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
# Use 'income' for hue only if it exists in pca_df
if 'income' in pca_data.columns:
    sns.scatterplot(x='PC1', y='PC2', hue='income', data=pca_data, palette='bright')
else:
    sns.scatterplot(x='PC1', y='PC2', data=pca_data, palette='bright') # Plot without hue if 'income' is missing
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

import matplotlib.cm as cm # Import the cm module for colormaps

def correlation_matrix(data):
    # Seleccionar solo columnas numéricas para calcular la correlación
    numeric_data = data.select_dtypes(include=['int64', 'float64'])

    fig = plt.figure(figsize=(16, 12))
    ax1 = fig.add_subplot(111)
    cmap = cm.get_cmap("jet", 30)

    # Calcular correlación en datos numéricos solamente
    cax = ax1.imshow(numeric_data.corr(), interpolation="nearest", cmap=cmap)
    ax1.grid(True)

    plt.title("Correlación de características del conjunto de datos", fontsize=15)
    labels = numeric_data.columns  # Usar etiquetas de datos numéricos

    ax1.set_xticks(np.arange(len(labels)))
    ax1.set_yticks(np.arange(len(labels)))
    ax1.set_xticklabels(labels, fontsize=9)
    ax1.set_yticklabels(labels, fontsize=9)

    # Añadir barra de color
    fig.colorbar(cax, ticks=[0.1 * i for i in range(-11, 11)])
    plt.show()


correlation_matrix(data)

plt.figure(figsize=(10, 6))
plt.bar(range(1, len(explained_variance) + 1), explained_variance, color='skyblue')
plt.title('Varianza Explicada por Cada Componente Principal')
plt.xlabel('Componente Principal')
plt.ylabel('Varianza Explicada')
plt.grid(True)
plt.show()

pca_loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(pca.n_components_)], index=x_cols)
print(pca_loadings)

# Gráfico de cargas de variables en los primeros dos componentes principales
plt.figure(figsize=(12, 6))
sns.heatmap(pca_loadings[['PC1', 'PC2']], annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Cargas de Variables en los Primeros Dos Componentes Principales')
plt.show()

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_cols)

# Aplicar PCA
pca = PCA(n_components=2)  # Elegir 2 componentes principales para visualizar en 2D
pca_result = pca.fit_transform(scaled_data)

# Crear un DataFrame con los resultados del PCA
pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Añadir las etiquetas de clase (income) para colorizar el gráfico
# Asegurarse de que 'native.country' está presente en el DataFrame original 'data'
pca_data['age'] = data['age']  # Add this line to include 'native.country'

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='age', data=pca_data, palette='bright')
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_cols)

# Aplicar PCA
pca = PCA(n_components=2)  # Elegir 2 componentes principales para visualizar en 2D
pca_result = pca.fit_transform(scaled_data)

# Crear un DataFrame con los resultados del PCA
pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Añadir las etiquetas de clase (income) para colorizar el gráfico
# Asegurarse de que 'native.country' está presente en el DataFrame original 'data'
pca_data['education'] = data['education']  # Add this line to include 'native.country'

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='education', data=pca_data, palette='bright')
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_cols)

# Aplicar PCA
pca = PCA(n_components=2)  # Elegir 2 componentes principales para visualizar en 2D
pca_result = pca.fit_transform(scaled_data)

# Crear un DataFrame con los resultados del PCA
pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Añadir las etiquetas de clase (income) para colorizar el gráfico
# Asegurarse de que 'native.country' está presente en el DataFrame original 'data'
pca_data['occupation'] = data['occupation']  # Add this line to include 'native.country'

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='occupation', data=pca_data, palette='bright')
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_cols)

# Aplicar PCA
pca = PCA(n_components=2)  # Elegir 2 componentes principales para visualizar en 2D
pca_result = pca.fit_transform(scaled_data)

# Crear un DataFrame con los resultados del PCA
pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Añadir las etiquetas de clase (income) para colorizar el gráfico
# Asegurarse de que 'native.country' está presente en el DataFrame original 'data'
pca_data['race'] = data['race']  # Add this line to include 'native.country'

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='race', data=pca_data, palette='bright')
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_cols)

# Aplicar PCA
pca = PCA(n_components=2)  # Elegir 2 componentes principales para visualizar en 2D
pca_result = pca.fit_transform(scaled_data)

# Crear un DataFrame con los resultados del PCA
pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Añadir las etiquetas de clase (income) para colorizar el gráfico
# Asegurarse de que 'native.country' está presente en el DataFrame original 'data'
pca_data['sex'] = data['sex']  # Add this line to include 'native.country'

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='sex', data=pca_data, palette='bright')
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

numeric_cols = data.select_dtypes(include=['int64', 'float64'])


scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_cols)

# Aplicar PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(scaled_data)

pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])


pca_data['native.country'] = data['native.country']  # Add this line to include 'native.country'

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='native.country', data=pca_data, palette='bright')
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Estandarizar los datos
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_cols)

# Aplicar PCA
pca = PCA(n_components=2)  # Elegir 2 componentes principales para visualizar en 2D
pca_result = pca.fit_transform(scaled_data)

# Crear un DataFrame con los resultados del PCA
pca_data = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Añadir las etiquetas de clase (income) para colorizar el gráfico
# Asegurarse de que 'native.country' está presente en el DataFrame original 'data'
pca_data['income'] = data['income']  # Add this line to include 'native.country'

# Visualizar los resultados del PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='income', data=pca_data, palette='bright')
plt.title('PCA de Datos')
plt.xlabel('Primer Componente Principal (PC1)')
plt.ylabel('Segundo Componente Principal (PC2)')
plt.show()

# Mostrar la varianza explicada por cada componente principal
explained_variance = pca.explained_variance_ratio_
print(f'Varianza explicada por el PC1: {explained_variance[0]:.2f}')
print(f'Varianza explicada por el PC2: {explained_variance[1]:.2f}')

"""K-MEANS"""

data.head()

data['income'] = data['income'].apply(lambda x: 1 if x == '>50K' else 0)

# Selección de características relevantes
data_for_clustering = data[['age', 'income']]

# Estandarización de los datos
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_for_clustering)

modelo_kmeans = KMeans(n_clusters=4, n_init=25, random_state=123)
modelo_kmeans.fit(X=data_scaled)

# Predecir clusters
y_predict = modelo_kmeans.predict(data_scaled)

# Handle non-numerical values (e.g., 'Unknown') in relevant columns
data = data.replace('Unknown', np.nan)
data = data.dropna()


data['income'] = data['income'].apply(lambda x: 1 if x == '>50K' else 0)


data_for_clustering = data[['age', 'income']]
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_for_clustering)

kmeans.labels_

import yellowbrick

# Handle non-numerical values and select relevant features
data_for_clustering = data[['age', 'income']]
data_for_clustering = data_for_clustering.replace('Unknown', np.nan).dropna()  # Handle missing/unknown values
data_for_clustering['income'] = data_for_clustering['income'].apply(lambda x: 1 if x == '>50K' else 0)

# Standardize the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_for_clustering)

# Apply KElbowVisualizer to the *scaled* numerical data
model = KMeans()
visualizer = yellowbrick.cluster.elbow.KElbowVisualizer(model, k=(1, 12))
visualizer.fit(data_scaled)  # Fit to the scaled numerical data
visualizer.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Determinar el número óptimo de clusters usando el método del codo
inertia = []
for n in range(1, 11):
    kmeans = KMeans(n_clusters=n, random_state=0)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)

# Graficar el método del codo
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Método del Codo para Número Óptimo de Clusters')
plt.xlabel('Número de Clusters')
plt.ylabel('Inercia')
plt.grid(True)
plt.show()

optimal_clusters = 3

kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
clusters = kmeans.fit_predict(data_scaled)

# Agregar los clusters a los datos originales
data['Cluster'] = clusters

!pip install plotly
import plotly.express as px # Import the plotly.express library

data['income'] = data['income'].apply(lambda x: 1 if x == '>50K' else 0)

# Selección de características relevantes
data_for_clustering = data[['age', 'income', 'capital.gain', 'hours.per.week']]

# Normalización de los datos
data_normalized = pd.DataFrame(normalize(data_for_clustering), columns=data_for_clustering.columns)

kmedia_4 = KMeans(n_clusters=4, init="k-means++", n_init=10, max_iter=300,
                  tol=0.0001, random_state=111, algorithm="elkan")

# Entrenamiento del modelo KMeans
kmedia_4.fit(data_normalized)
labels_4k = kmedia_4.labels_
centroides = kmedia_4.cluster_centers_

# Agregar etiquetas de clusters al DataFrame original
data["clusters_4k"] = labels_4k

fig = px.scatter_3d(data, x="age", y="capital.gain", z="hours.per.week", color="clusters_4k", width=800, height=800)

fig.show()

fig = plt.figure(figsize=(9,9))  # Crear la figura
ax = fig.add_subplot(111, projection="3d")  # Añadir un subplot 3D a la figura

x = data["age"]
y = data["income"]
# Replace 'sex' with a numerical column
z = data["hours.per.week"]

ax.set_xlabel("age")
ax.set_ylabel("income")
ax.set_zlabel("hours.per.week")  # Update z-axis label

# Usar las etiquetas de los clusters para colorear los puntos
ax.scatter(x, y, z, c=labels_4k, s=50, cmap="viridis")

plt.show()